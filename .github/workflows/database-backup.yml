name: Database Backup

on:
  schedule:
    # Run daily at 2 AM UTC (adjust timezone as needed)
    - cron: '0 2 * * *'
  workflow_dispatch: # Allow manual trigger
    inputs:
      backup_type:
        description: 'Type of backup'
        required: true
        default: 'full'
        type: choice
        options:
          - full
          - schema_only
          - data_only

env:
  # These will be set as GitHub Secrets
  DATABASE_URL: ${{ secrets.DATABASE_URL }}
  GOOGLE_DRIVE_FOLDER_ID: ${{ secrets.GOOGLE_DRIVE_FOLDER_ID }}
  GOOGLE_SERVICE_ACCOUNT_KEY: ${{ secrets.GOOGLE_SERVICE_ACCOUNT_KEY }}

jobs:
  backup:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Use Docker for PostgreSQL 17.x backup
      run: |
        # Install Docker and use PostgreSQL 17.x container for backup
        sudo apt-get update
        sudo apt-get install -y docker.io
        sudo systemctl start docker
        sudo docker pull postgres:17.6
        echo "Docker PostgreSQL 17.6 ready for backup"

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install google-api-python-client google-auth google-auth-oauthlib google-auth-httplib2 psycopg2-binary

    - name: Test configuration
      run: |
        python scripts/test_connection.py

    - name: Create backup directory
      run: mkdir -p backups

    - name: Generate backup filename
      id: backup_name
      run: |
        TIMESTAMP=$(date +%Y%m%d_%H%M%S)
        BACKUP_TYPE="${{ github.event.inputs.backup_type || 'full' }}"
        echo "filename=clove_db_backup_${BACKUP_TYPE}_${TIMESTAMP}.sql" >> $GITHUB_OUTPUT

    - name: Create database backup
      id: backup
      run: |
        BACKUP_FILE="backups/${{ steps.backup_name.outputs.filename }}"
        
        # Extract database connection details from DATABASE_URL
        DB_URL="$DATABASE_URL"
        DB_HOST=$(echo $DB_URL | sed 's/.*@\([^:]*\):.*/\1/')
        DB_PORT=$(echo $DB_URL | sed 's/.*:\([0-9]*\)\/.*/\1/')
        DB_USER=$(echo $DB_URL | sed 's/.*:\/\/\([^:]*\):.*/\1/')
        DB_PASS=$(echo $DB_URL | sed 's/.*:\/\/[^:]*:\([^@]*\)@.*/\1/')
        DB_NAME=$(echo $DB_URL | sed 's/.*\/\([^?]*\).*/\1/')
        
        case "${{ github.event.inputs.backup_type || 'full' }}" in
          "full")
            echo "Creating full database backup using PostgreSQL 17.6 Docker..."
            sudo docker run --rm -v $PWD/backups:/backups -e PGPASSWORD="$DB_PASS" postgres:17.6 pg_dump -h "$DB_HOST" -p "$DB_PORT" -U "$DB_USER" -d "$DB_NAME" --verbose --format=plain --file="/backups/${{ steps.backup_name.outputs.filename }}"
            ;;
          "schema_only")
            echo "Creating schema-only backup using PostgreSQL 17.6 Docker..."
            sudo docker run --rm -v $PWD/backups:/backups -e PGPASSWORD="$DB_PASS" postgres:17.6 pg_dump -h "$DB_HOST" -p "$DB_PORT" -U "$DB_USER" -d "$DB_NAME" --verbose --format=plain --schema-only --file="/backups/${{ steps.backup_name.outputs.filename }}"
            ;;
          "data_only")
            echo "Creating data-only backup using PostgreSQL 17.6 Docker..."
            sudo docker run --rm -v $PWD/backups:/backups -e PGPASSWORD="$DB_PASS" postgres:17.6 pg_dump -h "$DB_HOST" -p "$DB_PORT" -U "$DB_USER" -d "$DB_NAME" --verbose --format=plain --data-only --file="/backups/${{ steps.backup_name.outputs.filename }}"
            ;;
        esac
        
        # Get file size
        FILE_SIZE=$(du -h "$BACKUP_FILE" | cut -f1)
        echo "file_size=$FILE_SIZE" >> $GITHUB_OUTPUT
        echo "file_path=$BACKUP_FILE" >> $GITHUB_OUTPUT

    - name: Compress backup
      run: |
        BACKUP_FILE="${{ steps.backup.outputs.file_path }}"
        COMPRESSED_FILE="${BACKUP_FILE}.gz"
        gzip "$BACKUP_FILE"
        echo "file_path=$COMPRESSED_FILE" >> $GITHUB_OUTPUT
        echo "file_size=$(du -h "$COMPRESSED_FILE" | cut -f1)" >> $GITHUB_OUTPUT

    - name: Upload to Google Drive
      run: |
        python scripts/upload_backup_gdrive.py \
          --file "${{ steps.backup.outputs.file_path }}.gz" \
          --folder-id "$GOOGLE_DRIVE_FOLDER_ID" \
          --filename "${{ steps.backup_name.outputs.filename }}.gz"

    - name: Clean up old backups (keep last 30 days)
      run: |
        python scripts/cleanup_old_backups_gdrive.py \
          --folder-id "$GOOGLE_DRIVE_FOLDER_ID" \
          --keep-days 30

    - name: Backup Summary
      run: |
        echo "âœ… Database backup completed successfully!"
        echo "ğŸ“ File: ${{ steps.backup_name.outputs.filename }}.gz"
        echo "ğŸ“Š Size: ${{ steps.backup.outputs.file_size }}"
        echo "ğŸ•’ Time: $(date)"
        echo "â˜ï¸  Location: Google Drive folder $GOOGLE_DRIVE_FOLDER_ID"

    - name: Upload backup as artifact (for debugging)
      if: failure()
      uses: actions/upload-artifact@v4
      with:
        name: database-backup-failed
        path: backups/
        retention-days: 7
